{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internet Resources:\n",
    "\n",
    "[handson-ml/06_decision_trees.ipynb](https://github.com/ageron/handson-ml/blob/master/06_decision_trees.ipynb)  \n",
    "[Sefik Ilkin Serengil - A Step by Step CART Decision Tree Example](https://sefiks.com/2018/08/27/a-step-by-step-cart-decision-tree-example/)  \n",
    "[Google Developers - Tree Classifier from Scratch](https://www.youtube.com/watch?v=LDRbO9a6XPU&t=1s)  \n",
    "[Victor Zhou - A Simple Explanation of Gini Impurity](https://victorzhou.com/blog/gini-impurity/)\n",
    "\n",
    "\n",
    "Literature:  \n",
    "\n",
    "Aurelien geron hands on machine learning page 173"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     petal_length  petal_width  species\n",
       "0             1.4          0.2        0\n",
       "1             1.4          0.2        0\n",
       "2             1.3          0.2        0\n",
       "3             1.5          0.2        0\n",
       "4             1.4          0.2        0\n",
       "..            ...          ...      ...\n",
       "145           5.2          2.3        2\n",
       "146           5.0          1.9        2\n",
       "147           5.2          2.0        2\n",
       "148           5.4          2.3        2\n",
       "149           5.1          1.8        2\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# data set is the iris dataset from https://www.kaggle.com/arshid/iris-flower-dataset\n",
    "# we are only keeping features \"petal_length\" and \"petal_width\"\n",
    "df = pd.read_csv(\"data/iris.csv\").drop([\"sepal_width\", \"sepal_length\"], 1)\n",
    "\n",
    "# convert labels to numbers\n",
    "label_mappings = {label_str:i for i,label_str in enumerate(df[\"species\"].unique())}\n",
    "df.replace({\"species\":label_mappings}, inplace=True)\n",
    "\n",
    "# shuffle data\n",
    "permutation = np.random.permutation(df.index)\n",
    "X = np.array(df.drop([\"species\"], 1))[permutation]\n",
    "y = np.array(df[\"species\"])[permutation]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is decision tree ##\n",
    "Here's an example of a descision tree: the nodes hold questions, the last node of each branch (leaf nodes) holds a class, i.e prediction of the tree.\n",
    "![](https://miro.medium.com/max/690/1*xzF10JmR3K0rnZ8jtIHI_g.png)\n",
    "\n",
    "## How to build a decision tree ##\n",
    "Our goal is to create a tree that best fits our data. We will be creating nodes recursivly.  \n",
    "\n",
    "###### Creating Nodes ######\n",
    "When creating a node our goal is to find a \"question\" (e.g petal_length > 3? salary > 50k? hair color == blond?) with which we can \"best\" split up our data into two parts. Mathematically speaking, \"best\" means it minimizes some cost function. We'll look at one cost function later. \n",
    "\n",
    "Here's an **outline** of what's to do in pseudocode:       "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "question_function_(feature_of_sample, question_value):\n",
    "    if question_value is numeric:\n",
    "        return question_value >= feature_of_sample\n",
    "    else:\n",
    "        return question_value == feature_of_sample\n",
    "\n",
    "\n",
    "create_new_node(training_data)\n",
    "    for every feature in training samples:\n",
    "        for every unique_value in feature:\n",
    "            split_training_data = {\n",
    "                True: All samples for which question_function(sample[feature], unique_value) is True,\n",
    "                False: All samples for which question_function(sample[feature], unique_value) is False,\n",
    "            } \n",
    "        \n",
    "            calculate cost(split_training_data)\n",
    "    \n",
    "    create Node with unique_value as question_value that produces the lowest cost\n",
    "    \n",
    "    if max_depth is not reached and data is not pure (=all of same class) create child nodes:\n",
    "        Node.child[True] = create_new_node(split_training_data[True])\n",
    "        Node.child[False] = create_new_node(split_training_data[False])\n",
    "    else child nodes are leaf nodes \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider every value in every feature for every sample in given training data as a potential question. For each potential question we split our data according to the question and calculate the cost. Our final question for the node is the one that produces the lowest cost. Then we create two new child nodes, each for one set of the split training data.\n",
    "When we've reached the end of a branch we create a leaf node that will just reaturn the majority class of its training data when given a query\n",
    "\n",
    "###### Cost function ######\n",
    "The cost function used in this example is the gini index. It measures the likelihood of missclassifying a sample. It's defined as:  \n",
    "\n",
    "$$ \\text{gini} = \\sum p_i \\cdot (1-p_i) = 1 - \\sum p_i^2 \\in [0,1]$$\n",
    "\n",
    "where $i$ is the ith class and $p_i = \\frac{\\text{# of samples in class i}}{\\text{# of samples in all classes}}$ is the probability of of class $i$. 1 is the worst value, 0 the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " if feature 0 is greater or equal to 3.0:\n",
      "\t if feature 1 is greater or equal to 1.8:\n",
      "\t\t if feature 0 is greater or equal to 4.9:\n",
      "\t\t\t Sample is of class 2\n",
      "\t\t else:\n",
      "\t\t\t Sample is of class 2\n",
      "\t else:\n",
      "\t\t if feature 0 is greater or equal to 5.0:\n",
      "\t\t\t if feature 1 is greater or equal to 1.6:\n",
      "\t\t\t\t if feature 0 is greater or equal to 5.8:\n",
      "\t\t\t\t\t Sample is of class 2\n",
      "\t\t\t\t else:\n",
      "\t\t\t\t\t Sample is of class 1\n",
      "\t\t\t else:\n",
      "\t\t\t\t Sample is of class 2\n",
      "\t\t else:\n",
      "\t\t\t if feature 1 is greater or equal to 1.7:\n",
      "\t\t\t\t Sample is of class 2\n",
      "\t\t\t else:\n",
      "\t\t\t\t Sample is of class 1\n",
      " else:\n",
      "\t Sample is of class 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def gini(distributions): \n",
    "    # distributions is a list where each element represents the amount of Samples belonging to each class\n",
    "    # example [20, 30, 10] -> 20 x Sample belonging to class 0 \n",
    "                        #  -> 30 x Sample belonging to class 1\n",
    "                        #  -> 10 x Sample belonging to class 2\n",
    "    if sum(distributions) == 0: # if there are no samples for the node\n",
    "        return 0\n",
    "    num_samples = sum(distributions)\n",
    "    impurity = 1\n",
    "    for i in distributions:\n",
    "        impurity -= (i/num_samples)**2\n",
    "    return impurity\n",
    " \n",
    "\n",
    "# question function of node\n",
    "def ask(descision_value, ask_value):\n",
    "    if isinstance(ask_value, int) or isinstance(ask_value, float):    \n",
    "        return ask_value >= descision_value\n",
    "    else:\n",
    "        return ask_value == descision_value\n",
    "\n",
    "\n",
    "    \n",
    "# end of branch\n",
    "class TreeNodeEnd():\n",
    "    def __init__(self, labels, distribution, depth):\n",
    "        self.label = int(np.bincount(labels).argmax()) # most frequent label in labels\n",
    "        self.distribution = distribution\n",
    "        self.depth = depth\n",
    "        self.gini_score = gini(self.distribution)\n",
    "    \n",
    "    def predict(self, value):\n",
    "        return self.label # majority class of training data\n",
    "    \n",
    "    def print_tree(self):\n",
    "        print(\"\\t\"*self.depth, \"Sample is of class {}\".format(self.label))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# decision node\n",
    "class TreeNode():\n",
    "    def __init__(self, data, labels, distrb, gini, descision_value, feature, depth):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.distribution = distrb\n",
    "        self.gini_score = gini\n",
    "        self.feature = feature\n",
    "        self.descision_value = descision_value\n",
    "        self.depth = depth\n",
    "        self.child = {True:None, False:None}\n",
    "    \n",
    "    def predict(self, value):            \n",
    "        return self.child[ask(self.descision_value, value[self.feature])].predict(value)\n",
    "        \n",
    "    def print_tree(self):\n",
    "        cond = \"is greater or equal to\" if isinstance(self.descision_value, int) or isinstance(self.descision_value, float) else \"is equal to\" \n",
    "        print(\"\\t\"*self.depth, \"if feature {} {} {}:\".format(self.feature, cond, self.descision_value))\n",
    "        self.child[True].print_tree()\n",
    "        print(\"\\t\"*self.depth, \"else:\")\n",
    "        self.child[False].print_tree()\n",
    "   \n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "class DescisionTreeClassifier:\n",
    "    def __init__(self, max_depth = None):\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def find_lowest_gini(self, data, labels):\n",
    "        # find the featrue value (=question) that produces the lowest gini score and partition the data according to it\n",
    "        \n",
    "        # only get unique feature values - no need to check the same feature value twice\n",
    "        unique_column_values = {i:list(np.unique(column)) for i,column in enumerate(data.T) } \n",
    "        gini_score = 1\n",
    "        \n",
    "        # for each feature\n",
    "        for column in unique_column_values:\n",
    "            # for each value in this feature\n",
    "            for unique_value in unique_column_values[column]: \n",
    "                # distrb is a list where each element represents the amount of Samples belonging to each class\n",
    "                # example [20, 30, 10] -> 20 x Sample belonging to class 0 \n",
    "                        #  -> 30 x Sample belonging to class 1\n",
    "                        #  -> 10 x Sample belonging to class 2\n",
    "                distrb = {True:[0] * self.class_count, False:[0] * self.class_count}\n",
    "                \n",
    "                # split data into two groups: True and False\n",
    "                split_data = {True:[], False:[]}\n",
    "                split_labels = {True:[], False:[]}\n",
    "                for i in range(len(data)):\n",
    "                    # evaluate question with current feature value\n",
    "                    branch = ask(unique_value, data.T[column][i])\n",
    "                    split_data[branch].append(data[i])\n",
    "                    split_labels[branch].append(labels[i])\n",
    "                    distrb[branch][labels[i]] += 1\n",
    "                \n",
    "                # gini score is the weighted sum of both branches\n",
    "                new_gini = (sum(distrb[True]) / len(data) * gini(distrb[True])) + (sum(distrb[False]) \\\n",
    "                                                                                   / len(data) * gini(distrb[False]))\n",
    "                \n",
    "                #convert from list to np.array\n",
    "                split_data = {True:np.array(split_data[True]), False:np.array(split_data[False])}\n",
    "                split_labels = {True:np.array(split_labels[True]), False:np.array(split_labels[False])}\n",
    "                \n",
    "                # check if question has lower gini score\n",
    "                if new_gini < gini_score:\n",
    "                    gini_score = new_gini\n",
    "                    # split data and labels, distribution, question value, feature\n",
    "                    lowest_gini_data = split_data, split_labels, distrb, gini_score, unique_value, column\n",
    "        return lowest_gini_data \n",
    "            \n",
    "    \n",
    "    def create_next_node(self, data, labels, distribution, depth=0):\n",
    "        # recursive function, builds the tree\n",
    "        label = int(np.bincount(labels).argmax()) # most often occouring label\n",
    "        \n",
    "        # feature value that produces the lowest gini score and partition data accordinglý\n",
    "        split_data, split_labels, distrb, gini_score, descision_value, feature = self.find_lowest_gini(data, labels)\n",
    "        \n",
    "        # data has to be splitable \n",
    "        if list(distrb[True]) != distribution and list(distrb[False]) != distribution:\n",
    "            node = TreeNode(data, labels, distribution, gini_score, descision_value, feature, depth)\n",
    "            \n",
    "            # need to create nodes for both outcomes \n",
    "            for branch in [True, False]:\n",
    "                # check if depth has been reched\n",
    "                depth_reached = False if self.max_depth is None else True if depth+1 == self.max_depth else False\n",
    "                # node is pure if all training data belongs to one class only\n",
    "                node_pure = distrb[branch].count(0) >= self.class_count-1\n",
    "                # create child nodes\n",
    "                if depth_reached or node_pure:\n",
    "                    node.child[branch] = TreeNodeEnd(split_labels[branch], distrb[False], depth+1)\n",
    "                else:\n",
    "                    # recursion\n",
    "                    node.child[branch] = self.create_next_node(split_data[branch], split_labels[branch], distrb[branch], depth+1)\n",
    "            \n",
    "        else: # otherwise we are at the end of the branch\n",
    "            node = TreeNodeEnd(labels, distribution, depth)\n",
    "        return node\n",
    "        \n",
    "    \n",
    "    def fit(self, data, labels):\n",
    "        # start recursive process of creating nodes\n",
    "        self.class_count = len(np.unique(labels))\n",
    "        distribution = [0] * len(np.unique(labels))\n",
    "        for i in range(len(labels)):\n",
    "            distribution[labels[i]] += 1\n",
    "        self.root = self.createNextNode(data, labels, distribution)\n",
    "        \n",
    "    def predict(self, sample):\n",
    "        # recursive function, predicts class of sample\n",
    "        return self.root.predict(sample)\n",
    "    \n",
    "    def print_tree(self):\n",
    "        # recursive function, prints decision questions of the tree\n",
    "        self.root.print_tree()\n",
    "    \n",
    "    \n",
    "    \n",
    "clf = DescisionTreeClassifier(max_depth=None)\n",
    "clf.fit(X, y)\n",
    "clf.print_tree()\n",
    "clf.predict([1,2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial-env",
   "language": "python",
   "name": "tutorial-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
